# AT Agent Accessibility Tests for test-website

stages:
  - test

accessibility-tests:
  stage: test
  image: node:20

  variables:
    PROVIDER: "openai"
    HEADLESS: "true"
    CONTINUE_ON_FAILURE: "true"
    # Dashboard integration (optional - set these in GitLab CI/CD Settings > Variables)
    # DASHBOARD_URL: "https://at-agent-dashboard.fly.dev"  # Or your own dashboard URL
    # DASHBOARD_API_KEY: <set in CI/CD variables, masked>

  before_script:
    # Install jq for JSON parsing
    - apt-get update && apt-get install -y jq

    # Install test-website dependencies and start the app
    - npm install
    - PORT=3000 npm start &
    - sleep 5
    - npx wait-on http://localhost:3000 --timeout 60000

    # Clone and set up at-agent (uses AT_AGENT_PAT variable for private repo access)
    - git clone --branch langgraph-agent https://oauth2:${AT_AGENT_PAT}@github.com/justanothernoob4648/at-agent.git /at-agent
    - cd /at-agent && npm install
    - npx playwright install --with-deps chromium

  script:
    - |
      cd /at-agent

      RESULTS="[]"
      PASSED=0
      FAILED=0
      TOTAL=0

      # Read the test config file
      TEST_FILE="${CI_PROJECT_DIR}/a11y-tests.json"

      # Get number of tests
      NUM_TESTS=$(jq 'length' "$TEST_FILE")
      echo "Running $NUM_TESTS accessibility tests..."
      echo ""

      # Start live tracking if dashboard is configured
      TEST_RUN_ID=""
      if [ -n "$DASHBOARD_URL" ] && [ -n "$DASHBOARD_API_KEY" ]; then
        echo "Starting live tracking on dashboard..."
        TESTS_PAYLOAD=$(cat "$TEST_FILE")
        LIVE_PAYLOAD=$(jq -n \
          --arg platform "gitlab-ci" \
          --arg jobName "${CI_PROJECT_PATH:-}/${CI_JOB_NAME:-}" \
          --arg buildNumber "${CI_PIPELINE_ID:-}" \
          --arg buildUrl "${CI_PIPELINE_URL:-}" \
          --arg branch "${CI_COMMIT_REF_NAME:-}" \
          --arg commit "${CI_COMMIT_SHA:-}" \
          --argjson totalTests "$NUM_TESTS" \
          --argjson tests "$TESTS_PAYLOAD" \
          '{platform: $platform, jobName: $jobName, buildNumber: $buildNumber, buildUrl: $buildUrl, branch: $branch, commit: $commit, totalTests: $totalTests, tests: $tests}')

        LIVE_RESPONSE=$(curl -s -X POST \
          -H "Content-Type: application/json" \
          -H "X-API-Key: $DASHBOARD_API_KEY" \
          -d "$LIVE_PAYLOAD" \
          "${DASHBOARD_URL%/}/api/live/start" || echo '{"error": "failed"}')

        TEST_RUN_ID=$(echo "$LIVE_RESPONSE" | jq -r '.testRunId // empty')
        if [ -n "$TEST_RUN_ID" ]; then
          echo "Live tracking started: ${DASHBOARD_URL%/}/runs/$TEST_RUN_ID"
          export TEST_RUN_ID
          export DASHBOARD_URL
          export DASHBOARD_API_KEY
        else
          echo "Warning: Could not start live tracking"
        fi
        echo ""
      fi

      # Loop through each test by index
      for i in $(seq 0 $((NUM_TESTS - 1))); do
        TOTAL=$((TOTAL + 1))

        # Extract url and goal using jq
        url=$(jq -r ".[$i].url" "$TEST_FILE")
        goal=$(jq -r ".[$i].goal" "$TEST_FILE")

        echo "==========================================="
        echo "Test $TOTAL of $NUM_TESTS"
        echo "URL: $url"
        echo "Goal: $goal"
        echo "==========================================="

        # Run the test and capture output
        set +e
        # Pass dashboard env vars and --live flag if configured
        LIVE_ARGS=""
        if [ -n "$DASHBOARD_URL" ] && [ -n "$DASHBOARD_API_KEY" ] && [ -n "$TEST_RUN_ID" ]; then
          export TEST_INDEX="$i"
          LIVE_ARGS="--live"
        fi
        OUTPUT=$(npm run start:agent-cli -- "$url" "$goal" "$PROVIDER" --json $LIVE_ARGS 2>&1)
        EXIT_CODE=$?
        set -e

        # Save output to temp file for debugging
        echo "$OUTPUT" > /tmp/test_output_$i.txt

        # Extract complete JSON by finding matching braces (like GitHub Actions does)
        JSON_RESULT=""
        if echo "$OUTPUT" | grep -q '{"url"'; then
          # Find the start of JSON and extract the complete object
          JSON_START=$(echo "$OUTPUT" | grep -n '{"url"' | head -1 | cut -d: -f1)
          if [ -n "$JSON_START" ]; then
            # Get everything from JSON start to end of output, then use Python to extract valid JSON
            TAIL_OUTPUT=$(echo "$OUTPUT" | tail -n +$JSON_START)
            JSON_RESULT=$(python3 -c "
import sys
import json

text = '''$TAIL_OUTPUT'''
# Find the JSON object by matching braces
start = text.find('{\"url\"')
if start >= 0:
    brace_count = 0
    end = -1
    for i, c in enumerate(text[start:]):
        if c == '{':
            brace_count += 1
        elif c == '}':
            brace_count -= 1
            if brace_count == 0:
                end = start + i + 1
                break
    if end > 0:
        try:
            obj = json.loads(text[start:end])
            print(json.dumps(obj))
        except:
            pass
" 2>/dev/null || echo "")
          fi
        fi

        # Check if we got valid JSON, if not create a failure result
        if [ -z "$JSON_RESULT" ] || ! echo "$JSON_RESULT" | jq . > /dev/null 2>&1; then
          # Try to extract error message from output
          ERROR_MSG=$(echo "$OUTPUT" | grep -i "error\|failed\|exception" | head -1 | tr -d '"' || echo "Could not parse agent output")
          JSON_RESULT=$(jq -n --arg url "$url" --arg goal "$goal" --arg err "$ERROR_MSG" '{url: $url, goal: $goal, success: false, error: $err, steps: [], violations: []}')
        fi

        # Get success status
        SUCCESS=$(echo "$JSON_RESULT" | jq -r '.success // false')

        if [ "$SUCCESS" = "true" ]; then
          PASSED=$((PASSED + 1))
          REASON=$(echo "$JSON_RESULT" | jq -r '.reason // "Test completed successfully"')
          echo "PASSED: $REASON"
        else
          FAILED=$((FAILED + 1))
          ERROR=$(echo "$JSON_RESULT" | jq -r '.error // .reason // "Unknown error"')
          echo "FAILED: $ERROR"
        fi

        # Add to results array
        RESULTS=$(echo "$RESULTS" | jq --argjson r "$JSON_RESULT" '. + [$r]')

        echo ""
      done

      # Write results
      echo "$RESULTS" > "${CI_PROJECT_DIR}/accessibility-results.json"

      # Complete live tracking
      if [ -n "$TEST_RUN_ID" ] && [ -n "$DASHBOARD_URL" ] && [ -n "$DASHBOARD_API_KEY" ]; then
        echo "Completing live tracking..."
        COMPLETE_PAYLOAD=$(jq -n \
          --arg testRunId "$TEST_RUN_ID" \
          --argjson results "$RESULTS" \
          --argjson totalDuration 0 \
          '{testRunId: $testRunId, results: $results, totalDuration: $totalDuration}')

        curl -s -X POST \
          -H "Content-Type: application/json" \
          -H "X-API-Key: $DASHBOARD_API_KEY" \
          -d "$COMPLETE_PAYLOAD" \
          "${DASHBOARD_URL%/}/api/live/complete" > /dev/null 2>&1 || true

        echo "Results available at: ${DASHBOARD_URL%/}/runs/$TEST_RUN_ID"
      fi

      echo "==========================================="
      echo "Summary"
      echo "==========================================="
      echo "Total: $TOTAL"
      echo "Passed: $PASSED"
      echo "Failed: $FAILED"
      echo "==========================================="

      # Exit with failure if any tests failed
      if [ "$FAILED" -gt 0 ]; then
        exit 1
      fi

  artifacts:
    when: always
    paths:
      - accessibility-results.json
    expire_in: 30 days
